{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"/Users/tommydenezza/Desktop/DeepLearning/LOBnet/data/BenchmarkDatasets_csv/BenchmarkDatasets/NoAuction/1.NoAuction_Zscore/NoAuction_Zscore_Training\"\n",
    "csv_files = [f for f in os.listdir(datapath) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_windows(df, window_size=100):\n",
    "    \"\"\"\n",
    "    Split a Pandas DataFrame into non-overlapping windows.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame to be split.\n",
    "    window_size (int): The size of each window (number of columns).\n",
    "    \n",
    "    Returns:\n",
    "    list of pandas.DataFrame: A list of DataFrame windows.\n",
    "    \"\"\"\n",
    "    num_windows = df.shape[1] // window_size\n",
    "    windows = []\n",
    "    for i in range(num_windows):\n",
    "        start = i * window_size\n",
    "        end = start + window_size\n",
    "        windows.append(df.iloc[:, start:end])\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on file 0\n",
      "Working on file 1\n",
      "Working on file 2\n",
      "Working on file 3\n",
      "Working on file 4\n",
      "Working on file 5\n",
      "Working on file 6\n",
      "Working on file 7\n",
      "Working on file 8\n"
     ]
    }
   ],
   "source": [
    "X_train_lst = []\n",
    "Y_train_lst = []\n",
    "for i,f in enumerate(csv_files):\n",
    "    print(f\"Working on file {i}\")\n",
    "    df = pd.read_csv(os.path.join(datapath, f), header=None)\n",
    "    X_train_lst += split_into_windows(df.iloc[0:40,:], window_size=100)\n",
    "    Y_train_lst += split_into_windows(df.iloc[-5:,:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array(X_train_lst).swapaxes(1,2)\n",
    "\n",
    "# # Naively ignore all information about the orderbook in order to get a baseline. Here, we are looking at the deviation from the end of the \n",
    "# # window to a point 10 events into the future. See pg 13 of https://arxiv.org/pdf/1705.03233\n",
    "# Y_train = np.array(Y_train_lst)[:,-1,-1] \n",
    "\n",
    "# encoder = OneHotEncoder(sparse_output=False)\n",
    "# Y_train = encoder.fit_transform(Y_train.reshape(-1,1)) \n",
    "\n",
    "X_data = np.array(X_train_lst).swapaxes(1,2)\n",
    "\n",
    "# Naively ignore all information about the orderbook in order to get a baseline. Here, we are looking at the deviation from the end of the \n",
    "# window to a point 10 events into the future. See pg 13 of https://arxiv.org/pdf/1705.03233\n",
    "Y_data = np.array(Y_train_lst)[:,-1,-1] \n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "Y_data = encoder.fit_transform(Y_data.reshape(-1,1)) \n",
    "\n",
    "split_point = int(0.75*len(X_data))\n",
    "X_train, X_test = X_data[split_point:], X_data[:split_point]\n",
    "Y_train, Y_test = Y_data[split_point:], Y_data[:split_point]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveLOBCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NaiveLOBCNN, self).__init__()\n",
    "\n",
    "        # Conv block see page 5 of https://arxiv.org/pdf/1808.03668\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=16,kernel_size=(1, 2),stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=(1, 2),stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(p=0.3) # Should have a conversation about what our desired dropout is, set to 3/10 rn\n",
    "\n",
    "        self.flatten_size = 32 * 10 * 25\n",
    "        \n",
    "        # FC \n",
    "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 3)  # 3 classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add channel dimension if not present\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        # Convolution blocks\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, self.flatten_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderBookDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = OrderBookDataset(X_train, Y_train)\n",
    "test_dataset = OrderBookDataset(X_test, Y_test)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0  # Increase if you need parallel loading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 133/133 [00:01<00:00, 103.74it/s, loss=1.0907, acc=38.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Average Loss: 1.0907\n",
      "Accuracy: 38.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 133/133 [00:01<00:00, 104.93it/s, loss=1.0771, acc=42.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Average Loss: 1.0771\n",
      "Accuracy: 42.36%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 133/133 [00:01<00:00, 101.59it/s, loss=1.0511, acc=43.33%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Average Loss: 1.0511\n",
      "Accuracy: 43.33%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 133/133 [00:01<00:00, 104.28it/s, loss=1.0371, acc=46.78%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Average Loss: 1.0371\n",
      "Accuracy: 46.78%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 133/133 [00:01<00:00, 109.33it/s, loss=1.0229, acc=47.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Average Loss: 1.0229\n",
      "Accuracy: 47.96%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 133/133 [00:01<00:00, 110.76it/s, loss=1.0087, acc=50.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Average Loss: 1.0087\n",
      "Accuracy: 50.05%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 133/133 [00:01<00:00, 110.95it/s, loss=0.9946, acc=52.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Average Loss: 0.9946\n",
      "Accuracy: 52.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 133/133 [00:01<00:00, 111.87it/s, loss=0.9930, acc=52.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Average Loss: 0.9930\n",
      "Accuracy: 52.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 133/133 [00:01<00:00, 103.08it/s, loss=1.0050, acc=52.08%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Average Loss: 1.0050\n",
      "Accuracy: 52.08%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 133/133 [00:01<00:00, 109.91it/s, loss=0.9689, acc=56.37%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "Average Loss: 0.9689\n",
      "Accuracy: 56.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = NaiveLOBCNN()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    for batch_X, batch_y in pbar:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(batch_y.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss/len(pbar):.4f}',\n",
    "            'acc': f'{100 * correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f'\\nEpoch {epoch+1} Summary:')\n",
    "    print(f'Average Loss: {epoch_loss:.4f}')\n",
    "    print(f'Accuracy: {epoch_acc:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Test Predictions: 100%|██████████| 397/397 [00:00<00:00, 546.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 40.74%\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0  # Increase if needed for parallel data loading\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "pred_batches = tqdm(test_dataloader, desc='Running Test Predictions')\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for X_batch, y_batch in pred_batches:\n",
    "    \n",
    "    X_batch = X_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "    \n",
    "    test_output = model(X_batch)\n",
    "    _, test_pred = torch.max(test_output.data, 1)  # Predicted class indices\n",
    "    \n",
    "    if y_batch.ndim > 1:\n",
    "        _, test_labels = torch.max(y_batch.data, 1)\n",
    "    else:\n",
    "        test_labels = y_batch \n",
    "    \n",
    "    all_predictions.extend(test_pred.cpu().numpy())\n",
    "    all_labels.extend(test_labels.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "accuracy = (all_predictions == all_labels).mean() * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 ... 1 1 1]\n",
      "[0 2 2 ... 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions)\n",
    "print(all_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
